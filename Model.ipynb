{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for Decentralized Multi-Robot Path Planning In Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset_Generator import DatasetGenerator \n",
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU setups\n",
    "\n",
    "# Optional: Force deterministic behavior on GPU (may impact performance)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [[0 for _ in range(20)] for _ in range(20)]  # Generate grid map\n",
    "dataset_generator = DatasetGenerator(num_cases=5000, num_agents=6, grid=grid)\n",
    "# cases = dataset_generator.generate_cases()\n",
    "# dataset_generator.save_cases_to_file(cases, \"dataset.json\")\n",
    "# print(f\"Generated and saved {len(cases)} cases.\")\n",
    "cases = dataset_generator.load_cases_from_file(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import Preprocessing\n",
    "p = Preprocessing(grid,cases,3)\n",
    "data_tensors  = p.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data_tensors.txt\", 'w') as file:\n",
    "#     file.write(str(data_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoder import Encode\n",
    "encoder = Encode(data_tensors,6)\n",
    "encoded_tensors = encoder.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"encoded_tensors.txt\", 'w') as file:\n",
    "#     file.write(str(encoded_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Adjacency_Matrix import adj_mat\n",
    "adj = adj_mat(cases,3)\n",
    "adj_matrices = adj.get_adj_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Adj_Matrices.txt\", 'w') as file:\n",
    "#     file.write(str(adj_matrices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\anaconda3\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "from GNN_file import Communication_GNN\n",
    "comm_gnn = Communication_GNN(encoded=encoded_tensors, adj_mat=adj_matrices, num_of_agents=6)\n",
    "gnn_features = comm_gnn.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"GNN_Features.txt\", 'w') as file:\n",
    "#     file.write(str(gnn_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Extarct_Actions import Action_Extractor\n",
    "action_extractor = Action_Extractor(cases, 6)\n",
    "actions = action_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_features = torch.cat([gnn_features[key][subkey] for key in gnn_features for subkey in gnn_features[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.tensor([actions[key][subkey] for key in actions for subkey in actions[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_gnn_features = []\n",
    "temp = []\n",
    "c = 1\n",
    "for i in range(len(gnn_features)):\n",
    "    temp.append(gnn_features[i].tolist())\n",
    "    if c % 6 == 0:\n",
    "        reshaped_gnn_features.append(torch.Tensor(temp))\n",
    "        temp = []\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(len(actions)):\n",
    "    dataset.append((reshaped_gnn_features[i],actions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ActionMLP(nn.Module):\n",
    "    def __init__(self, input_dim=128, hidden_dim=128, num_actions=5):\n",
    "        super(ActionMLP, self).__init__()\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_actions)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.dropout = nn.Dropout(0.2)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        x = self.dropout(x)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        x = self.dropout(x)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        x = self.fc3(x)\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MLP_Action import ActionMLP\n",
    "mlp = ActionMLP(input_dim= 128, hidden_dim= 128, num_actions= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "class Generate_Model:\n",
    "    def __init__(self,model, dataset, num_epochs = 150):\n",
    "        self.model = model\n",
    "        self.num_epochs = num_epochs\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        torch.manual_seed(seed)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        self.model.to(self.device)\n",
    "        train_size = int(0.7 * len(dataset))\n",
    "        val_size = int(0.15 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "        \n",
    "    def evaluate_model(self, model, dataloader, device, criterion):\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for gnn_feature, target_actions in dataloader:\n",
    "                # observations: [batch_size, num_agents, channels, height, width]\n",
    "                batch_size, num_agents = gnn_feature.shape[0], gnn_feature.shape[1]\n",
    "                # Flatten observations: [batch_size*num_agents, channels, height, width]\n",
    "                gnn_feature = gnn_feature.view(batch_size * num_agents,\n",
    "                                             gnn_feature.shape[2]).to(device)\n",
    "                # Flatten target actions: [batch_size*num_agents]\n",
    "                target_actions = target_actions.view(-1).to(device)\n",
    "                # Move edge_index to device\n",
    "                torch.manual_seed(seed)\n",
    "                # Forward pass\n",
    "                predictions = model(gnn_feature)  # [batch_size*num_agents, num_actions]\n",
    "                torch.manual_seed(seed)\n",
    "                loss = criterion(predictions, target_actions)\n",
    "                torch.manual_seed(seed)\n",
    "                total_loss += loss.item() * target_actions.size(0)\n",
    "                torch.manual_seed(seed)\n",
    "                # Compute accuracy\n",
    "                predicted_labels = torch.argmax(predictions, dim=1)\n",
    "                total_correct += (predicted_labels == target_actions).sum().item()\n",
    "                total_samples += target_actions.size(0)\n",
    "                \n",
    "        avg_loss = total_loss / total_samples\n",
    "        accuracy = total_correct / total_samples\n",
    "        return avg_loss, accuracy\n",
    "    \n",
    "    def train_model(self):\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for gnn_feature, target_action in self.train_loader:\n",
    "                seed = 42\n",
    "                torch.manual_seed(seed)\n",
    "                batch_size, num_agents = gnn_feature.shape[0], gnn_feature.shape[1]\n",
    "                gnn_feature = gnn_feature.view(batch_size * num_agents,\n",
    "                                                gnn_feature.shape[2]).to(self.device)\n",
    "                target_action = target_action.view(-1).to(self.device)\n",
    "                torch.manual_seed(seed)\n",
    "                optimizer.zero_grad()\n",
    "                torch.manual_seed(seed)\n",
    "                predictions = self.model(gnn_feature)\n",
    "                torch.manual_seed(seed)\n",
    "                loss = self.criterion(predictions, target_action)\n",
    "                torch.manual_seed(seed)\n",
    "                loss.backward()\n",
    "                torch.manual_seed(seed)\n",
    "                optimizer.step()\n",
    "                torch.manual_seed(seed)\n",
    "                running_loss += loss.item()\n",
    "                torch.manual_seed(seed)\n",
    "            avg_train_loss = running_loss / len(self.train_loader)\n",
    "            val_loss, val_accuracy = self.evaluate_model(self.model, self.val_loader, self.device, self.criterion)\n",
    "            scheduler.step(val_loss)\n",
    "            print(f\"Epoch [{epoch}/{self.num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f} - \"\n",
    "                  f\"LR: {scheduler.get_last_lr()[0]:.6f} - \"\n",
    "                  f\"Val Loss: {val_loss:.4f} - \"\n",
    "                  f\"Val Acc: {val_accuracy * 100:.2f}%\")\n",
    "        self.save_model()\n",
    "    def test_model(self):\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "        test_loss, test_accuracy = self.evaluate_model(self.model, self.test_loader, self.device, self.criterion)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "        return test_loss, test_accuracy\n",
    "    def save_model(self):\n",
    "        torch.save(self.model.state_dict(), \"trained_model_3.pth\")\n",
    "        print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Model_Generation import Generate_Model\n",
    "model = Generate_Model(model = mlp, dataset=dataset,num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] - Train Loss: 1.1091 - LR: 0.001000 - Val Loss: 1.0958 - Val Acc: 80.46%\n",
      "Epoch [2/200] - Train Loss: 1.0909 - LR: 0.001000 - Val Loss: 1.0903 - Val Acc: 80.98%\n",
      "Epoch [3/200] - Train Loss: 1.0863 - LR: 0.001000 - Val Loss: 1.0857 - Val Acc: 81.48%\n",
      "Epoch [4/200] - Train Loss: 1.0832 - LR: 0.001000 - Val Loss: 1.0851 - Val Acc: 81.57%\n",
      "Epoch [5/200] - Train Loss: 1.0807 - LR: 0.001000 - Val Loss: 1.0829 - Val Acc: 81.77%\n",
      "Epoch [6/200] - Train Loss: 1.0783 - LR: 0.001000 - Val Loss: 1.0826 - Val Acc: 81.80%\n",
      "Epoch [7/200] - Train Loss: 1.0763 - LR: 0.001000 - Val Loss: 1.0824 - Val Acc: 81.85%\n",
      "Epoch [8/200] - Train Loss: 1.0746 - LR: 0.001000 - Val Loss: 1.0835 - Val Acc: 81.76%\n",
      "Epoch [9/200] - Train Loss: 1.0730 - LR: 0.001000 - Val Loss: 1.0821 - Val Acc: 81.91%\n",
      "Epoch [10/200] - Train Loss: 1.0715 - LR: 0.001000 - Val Loss: 1.0829 - Val Acc: 81.84%\n",
      "Epoch [11/200] - Train Loss: 1.0700 - LR: 0.001000 - Val Loss: 1.0814 - Val Acc: 81.99%\n",
      "Epoch [12/200] - Train Loss: 1.0688 - LR: 0.001000 - Val Loss: 1.0816 - Val Acc: 81.98%\n",
      "Epoch [13/200] - Train Loss: 1.0675 - LR: 0.001000 - Val Loss: 1.0809 - Val Acc: 82.07%\n",
      "Epoch [14/200] - Train Loss: 1.0662 - LR: 0.001000 - Val Loss: 1.0810 - Val Acc: 82.07%\n",
      "Epoch [15/200] - Train Loss: 1.0649 - LR: 0.001000 - Val Loss: 1.0798 - Val Acc: 82.20%\n",
      "Epoch [16/200] - Train Loss: 1.0641 - LR: 0.001000 - Val Loss: 1.0795 - Val Acc: 82.27%\n",
      "Epoch [17/200] - Train Loss: 1.0631 - LR: 0.001000 - Val Loss: 1.0798 - Val Acc: 82.19%\n",
      "Epoch [18/200] - Train Loss: 1.0622 - LR: 0.001000 - Val Loss: 1.0794 - Val Acc: 82.26%\n",
      "Epoch [19/200] - Train Loss: 1.0615 - LR: 0.001000 - Val Loss: 1.0811 - Val Acc: 82.08%\n",
      "Epoch [20/200] - Train Loss: 1.0607 - LR: 0.001000 - Val Loss: 1.0801 - Val Acc: 82.16%\n",
      "Epoch [21/200] - Train Loss: 1.0597 - LR: 0.001000 - Val Loss: 1.0806 - Val Acc: 82.12%\n",
      "Epoch [22/200] - Train Loss: 1.0591 - LR: 0.001000 - Val Loss: 1.0800 - Val Acc: 82.19%\n",
      "Epoch [23/200] - Train Loss: 1.0582 - LR: 0.001000 - Val Loss: 1.0805 - Val Acc: 82.14%\n",
      "Epoch [24/200] - Train Loss: 1.0577 - LR: 0.001000 - Val Loss: 1.0802 - Val Acc: 82.17%\n",
      "Epoch [25/200] - Train Loss: 1.0570 - LR: 0.001000 - Val Loss: 1.0803 - Val Acc: 82.18%\n",
      "Epoch [26/200] - Train Loss: 1.0564 - LR: 0.001000 - Val Loss: 1.0806 - Val Acc: 82.12%\n",
      "Epoch [27/200] - Train Loss: 1.0560 - LR: 0.001000 - Val Loss: 1.0802 - Val Acc: 82.20%\n",
      "Epoch [28/200] - Train Loss: 1.0552 - LR: 0.001000 - Val Loss: 1.0800 - Val Acc: 82.20%\n",
      "Epoch [29/200] - Train Loss: 1.0548 - LR: 0.000500 - Val Loss: 1.0797 - Val Acc: 82.25%\n",
      "Epoch [30/200] - Train Loss: 1.0521 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.31%\n",
      "Epoch [31/200] - Train Loss: 1.0501 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.28%\n",
      "Epoch [32/200] - Train Loss: 1.0490 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.30%\n",
      "Epoch [33/200] - Train Loss: 1.0481 - LR: 0.000500 - Val Loss: 1.0789 - Val Acc: 82.31%\n",
      "Epoch [34/200] - Train Loss: 1.0474 - LR: 0.000500 - Val Loss: 1.0789 - Val Acc: 82.35%\n",
      "Epoch [35/200] - Train Loss: 1.0468 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.31%\n",
      "Epoch [36/200] - Train Loss: 1.0463 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.32%\n",
      "Epoch [37/200] - Train Loss: 1.0458 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.33%\n",
      "Epoch [38/200] - Train Loss: 1.0453 - LR: 0.000500 - Val Loss: 1.0791 - Val Acc: 82.32%\n",
      "Epoch [39/200] - Train Loss: 1.0450 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.35%\n",
      "Epoch [40/200] - Train Loss: 1.0446 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.33%\n",
      "Epoch [41/200] - Train Loss: 1.0442 - LR: 0.000500 - Val Loss: 1.0790 - Val Acc: 82.36%\n",
      "Epoch [42/200] - Train Loss: 1.0439 - LR: 0.000500 - Val Loss: 1.0791 - Val Acc: 82.35%\n",
      "Epoch [43/200] - Train Loss: 1.0436 - LR: 0.000500 - Val Loss: 1.0793 - Val Acc: 82.32%\n",
      "Epoch [44/200] - Train Loss: 1.0432 - LR: 0.000500 - Val Loss: 1.0792 - Val Acc: 82.35%\n",
      "Epoch [45/200] - Train Loss: 1.0429 - LR: 0.000250 - Val Loss: 1.0793 - Val Acc: 82.31%\n",
      "Epoch [46/200] - Train Loss: 1.0433 - LR: 0.000250 - Val Loss: 1.0790 - Val Acc: 82.35%\n",
      "Epoch [47/200] - Train Loss: 1.0424 - LR: 0.000250 - Val Loss: 1.0790 - Val Acc: 82.38%\n",
      "Epoch [48/200] - Train Loss: 1.0418 - LR: 0.000250 - Val Loss: 1.0790 - Val Acc: 82.38%\n",
      "Epoch [49/200] - Train Loss: 1.0414 - LR: 0.000250 - Val Loss: 1.0789 - Val Acc: 82.39%\n",
      "Epoch [50/200] - Train Loss: 1.0410 - LR: 0.000250 - Val Loss: 1.0789 - Val Acc: 82.39%\n",
      "Epoch [51/200] - Train Loss: 1.0407 - LR: 0.000250 - Val Loss: 1.0789 - Val Acc: 82.40%\n",
      "Epoch [52/200] - Train Loss: 1.0404 - LR: 0.000250 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [53/200] - Train Loss: 1.0401 - LR: 0.000250 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [54/200] - Train Loss: 1.0398 - LR: 0.000250 - Val Loss: 1.0789 - Val Acc: 82.39%\n",
      "Epoch [55/200] - Train Loss: 1.0396 - LR: 0.000250 - Val Loss: 1.0789 - Val Acc: 82.42%\n",
      "Epoch [56/200] - Train Loss: 1.0393 - LR: 0.000125 - Val Loss: 1.0789 - Val Acc: 82.41%\n",
      "Epoch [57/200] - Train Loss: 1.0402 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [58/200] - Train Loss: 1.0399 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [59/200] - Train Loss: 1.0395 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [60/200] - Train Loss: 1.0392 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [61/200] - Train Loss: 1.0390 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [62/200] - Train Loss: 1.0387 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [63/200] - Train Loss: 1.0385 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [64/200] - Train Loss: 1.0383 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [65/200] - Train Loss: 1.0381 - LR: 0.000125 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [66/200] - Train Loss: 1.0379 - LR: 0.000125 - Val Loss: 1.0789 - Val Acc: 82.39%\n",
      "Epoch [67/200] - Train Loss: 1.0377 - LR: 0.000125 - Val Loss: 1.0789 - Val Acc: 82.39%\n",
      "Epoch [68/200] - Train Loss: 1.0376 - LR: 0.000125 - Val Loss: 1.0789 - Val Acc: 82.39%\n",
      "Epoch [69/200] - Train Loss: 1.0374 - LR: 0.000125 - Val Loss: 1.0789 - Val Acc: 82.39%\n",
      "Epoch [70/200] - Train Loss: 1.0372 - LR: 0.000063 - Val Loss: 1.0789 - Val Acc: 82.40%\n",
      "Epoch [71/200] - Train Loss: 1.0379 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [72/200] - Train Loss: 1.0377 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [73/200] - Train Loss: 1.0375 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [74/200] - Train Loss: 1.0374 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [75/200] - Train Loss: 1.0372 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.39%\n",
      "Epoch [76/200] - Train Loss: 1.0371 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.39%\n",
      "Epoch [77/200] - Train Loss: 1.0369 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.39%\n",
      "Epoch [78/200] - Train Loss: 1.0368 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.40%\n",
      "Epoch [79/200] - Train Loss: 1.0367 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.39%\n",
      "Epoch [80/200] - Train Loss: 1.0366 - LR: 0.000063 - Val Loss: 1.0788 - Val Acc: 82.39%\n",
      "Epoch [81/200] - Train Loss: 1.0365 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.39%\n",
      "Epoch [82/200] - Train Loss: 1.0369 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [83/200] - Train Loss: 1.0368 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [84/200] - Train Loss: 1.0367 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [85/200] - Train Loss: 1.0366 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [86/200] - Train Loss: 1.0365 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [87/200] - Train Loss: 1.0364 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [88/200] - Train Loss: 1.0363 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.42%\n",
      "Epoch [89/200] - Train Loss: 1.0362 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [90/200] - Train Loss: 1.0361 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [91/200] - Train Loss: 1.0361 - LR: 0.000031 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [92/200] - Train Loss: 1.0360 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [93/200] - Train Loss: 1.0362 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [94/200] - Train Loss: 1.0361 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [95/200] - Train Loss: 1.0360 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [96/200] - Train Loss: 1.0360 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [97/200] - Train Loss: 1.0359 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.42%\n",
      "Epoch [98/200] - Train Loss: 1.0359 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.42%\n",
      "Epoch [99/200] - Train Loss: 1.0358 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [100/200] - Train Loss: 1.0358 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [101/200] - Train Loss: 1.0358 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.42%\n",
      "Epoch [102/200] - Train Loss: 1.0357 - LR: 0.000016 - Val Loss: 1.0788 - Val Acc: 82.42%\n",
      "Epoch [103/200] - Train Loss: 1.0357 - LR: 0.000008 - Val Loss: 1.0788 - Val Acc: 82.41%\n",
      "Epoch [104/200] - Train Loss: 1.0357 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [105/200] - Train Loss: 1.0357 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [106/200] - Train Loss: 1.0357 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [107/200] - Train Loss: 1.0356 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [108/200] - Train Loss: 1.0356 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [109/200] - Train Loss: 1.0356 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [110/200] - Train Loss: 1.0356 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [111/200] - Train Loss: 1.0355 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [112/200] - Train Loss: 1.0355 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [113/200] - Train Loss: 1.0355 - LR: 0.000008 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [114/200] - Train Loss: 1.0355 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.42%\n",
      "Epoch [115/200] - Train Loss: 1.0355 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [116/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [117/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [118/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [119/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [120/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [121/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [122/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [123/200] - Train Loss: 1.0354 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [124/200] - Train Loss: 1.0353 - LR: 0.000004 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [125/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [126/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [127/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [128/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [129/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [130/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [131/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [132/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [133/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [134/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [135/200] - Train Loss: 1.0353 - LR: 0.000002 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [136/200] - Train Loss: 1.0353 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [137/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [138/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [139/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [140/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [141/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [142/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [143/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [144/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [145/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [146/200] - Train Loss: 1.0352 - LR: 0.000001 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [147/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [148/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [149/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [150/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [151/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [152/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [153/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [154/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [155/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [156/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [157/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [158/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [159/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [160/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [161/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [162/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [163/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [164/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [165/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [166/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [167/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [168/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [169/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [170/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [171/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [172/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [173/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [174/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [175/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [176/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [177/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [178/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [179/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [180/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [181/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [182/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [183/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [184/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [185/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [186/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [187/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [188/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [189/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [190/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [191/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [192/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [193/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [194/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [195/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [196/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [197/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [198/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [199/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Epoch [200/200] - Train Loss: 1.0352 - LR: 0.000000 - Val Loss: 1.0787 - Val Acc: 82.43%\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0801, Test Accuracy: 82.28%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.08011870552934, 0.8228367649037814)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = ActionMLP(input_dim= 128, hidden_dim= 128, num_actions= 5)\n",
    "# state_dict = torch.load('trained_model_3.pth')\n",
    "# new_model.load_state_dict(state_dict)\n",
    "# new_model.eval()  # set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case  0\n",
      "case added\n",
      "case  1\n",
      "case added\n",
      "case  2\n",
      "case added\n",
      "case  3\n",
      "case added\n",
      "case  4\n",
      "case added\n",
      "case  5\n",
      "case added\n",
      "case  6\n",
      "case added\n",
      "case  7\n",
      "case added\n",
      "case  8\n",
      "case added\n",
      "case  9\n",
      "case added\n",
      "case  10\n",
      "case added\n",
      "case  11\n",
      "case added\n",
      "case  12\n",
      "case added\n",
      "case  13\n",
      "case added\n",
      "case  14\n",
      "case added\n",
      "case  15\n",
      "case added\n",
      "case  16\n",
      "case added\n",
      "case  17\n",
      "case added\n",
      "case  18\n",
      "case added\n",
      "case  19\n",
      "case added\n",
      "case  20\n",
      "case added\n",
      "case  21\n",
      "case added\n",
      "case  22\n",
      "case added\n",
      "case  23\n",
      "case added\n",
      "case  24\n",
      "case added\n",
      "case  25\n",
      "case added\n",
      "case  26\n",
      "case added\n",
      "case  27\n",
      "case added\n",
      "case  28\n",
      "case added\n",
      "case  29\n",
      "case added\n",
      "case  30\n",
      "case added\n",
      "case  31\n",
      "case added\n",
      "case  32\n",
      "case added\n",
      "case  33\n",
      "case added\n",
      "case  34\n",
      "case added\n",
      "case  35\n",
      "case added\n",
      "case  36\n",
      "case added\n",
      "case  37\n",
      "case added\n",
      "case  38\n",
      "case added\n",
      "case  39\n",
      "case added\n",
      "case  40\n",
      "case added\n",
      "case  41\n",
      "case added\n",
      "case  42\n",
      "case added\n",
      "case  43\n",
      "case added\n",
      "case  44\n",
      "case added\n",
      "case  45\n",
      "case added\n",
      "case  46\n",
      "case added\n",
      "case  47\n",
      "case added\n",
      "case  48\n",
      "case added\n",
      "case  49\n",
      "case added\n",
      "case  50\n",
      "case added\n",
      "case  51\n",
      "case added\n",
      "case  52\n",
      "case added\n",
      "case  53\n",
      "case added\n",
      "case  54\n",
      "case added\n",
      "case  55\n",
      "case added\n",
      "case  56\n",
      "case added\n",
      "case  57\n",
      "case added\n",
      "case  58\n",
      "case added\n",
      "case  59\n",
      "case added\n",
      "case  60\n",
      "case added\n",
      "case  61\n",
      "case added\n",
      "case  62\n",
      "case added\n",
      "case  63\n",
      "case added\n",
      "case  64\n",
      "case added\n",
      "case  65\n",
      "case added\n",
      "case  66\n",
      "case added\n",
      "case  67\n",
      "case added\n",
      "case  68\n",
      "case added\n",
      "case  69\n",
      "case added\n",
      "case  70\n",
      "case added\n",
      "case  71\n",
      "case added\n",
      "case  72\n",
      "case added\n",
      "case  73\n",
      "case added\n",
      "case  74\n",
      "case added\n",
      "case  75\n",
      "case added\n",
      "case  76\n",
      "case added\n",
      "case  77\n",
      "case added\n",
      "case  78\n",
      "case added\n",
      "case  79\n",
      "case added\n",
      "case  80\n",
      "case added\n",
      "case  81\n",
      "case added\n",
      "case  82\n",
      "case added\n",
      "case  83\n",
      "case added\n",
      "case  84\n",
      "case added\n",
      "case  85\n",
      "case added\n",
      "case  86\n",
      "case added\n",
      "case  87\n",
      "case added\n",
      "case  88\n",
      "case added\n",
      "case  89\n",
      "case added\n",
      "case  90\n",
      "case added\n",
      "case  91\n",
      "case added\n",
      "case  92\n",
      "case added\n",
      "case  93\n",
      "case added\n",
      "case  94\n",
      "case added\n",
      "case  95\n",
      "case added\n",
      "case  96\n",
      "case added\n",
      "case  97\n",
      "case added\n",
      "case  98\n",
      "case added\n",
      "case  99\n",
      "case added\n"
     ]
    }
   ],
   "source": [
    "new_dataset_generator = DatasetGenerator(num_cases=100, num_agents=6, grid=grid)\n",
    "eval_cases = new_dataset_generator.generate_cases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17, 9), (0, 16), (11, 16), (16, 14), (15, 1), (9, 6)]\n",
      "[(10, 16), (2, 3), (5, 17), (17, 0), (2, 6), (6, 13)]\n",
      "[[(17, 9), (16, 9), (16, 10), (15, 10), (15, 11), (14, 11), (14, 12), (13, 12), (13, 13), (12, 13), (12, 14), (11, 14), (11, 15), (10, 15), (10, 16)], [(0, 16), (0, 15), (0, 14), (0, 13), (0, 12), (0, 11), (0, 10), (0, 9), (0, 8), (0, 7), (0, 6), (0, 5), (0, 4), (1, 4), (1, 3), (2, 3)], [(11, 16), (10, 16), (9, 16), (8, 16), (7, 16), (6, 16), (5, 16), (5, 17)], [(16, 14), (16, 13), (16, 12), (16, 11), (16, 10), (16, 9), (16, 8), (16, 7), (16, 6), (16, 5), (16, 4), (16, 3), (16, 2), (16, 1), (16, 0), (17, 0)], [(15, 1), (14, 1), (13, 1), (12, 1), (11, 1), (10, 1), (9, 1), (8, 1), (7, 1), (6, 1), (6, 2), (5, 2), (5, 3), (4, 3), (4, 4), (3, 4), (3, 5), (2, 5), (2, 6)], [(9, 6), (9, 7), (9, 8), (9, 9), (9, 10), (8, 10), (8, 11), (7, 11), (7, 12), (6, 12), (6, 13)]]\n"
     ]
    }
   ],
   "source": [
    "print(eval_cases[0]['start_positions'])\n",
    "print(eval_cases[0]['goal_positions'])\n",
    "print(eval_cases[0]['paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 9)\n",
      "[10, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moham\\AppData\\Local\\Temp\\ipykernel_6560\\131704871.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prediction = model.model(torch.tensor(reshaped_gnn_features[0][robot],dtype=torch.float32).unsqueeze(0).to(device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trueeeeee\n",
      "(3, 6)\n",
      "[1, 5]\n",
      "trueeeeee\n",
      "(13, 10)\n",
      "[11, 16]\n",
      "trueeeeee\n",
      "(15, 5)\n",
      "[17, 17]\n",
      "trueeeeee\n",
      "(9, 6)\n",
      "[6, 3]\n",
      "trueeeeee\n",
      "(3, 6)\n",
      "[6, 10]\n",
      "trueeeeee\n",
      "(10, 13)\n",
      "[15, 14]\n",
      "trueeeeee\n",
      "(4, 17)\n",
      "[18, 6]\n",
      "trueeeeee\n",
      "(18, 9)\n",
      "[19, 5]\n",
      "trueeeeee\n",
      "(1, 7)\n",
      "[12, 15]\n",
      "trueeeeee\n",
      "(18, 5)\n",
      "[12, 11]\n",
      "trueeeeee\n",
      "(2, 16)\n",
      "[10, 3]\n",
      "trueeeeee\n",
      "(4, 18)\n",
      "[5, 16]\n",
      "trueeeeee\n",
      "(15, 5)\n",
      "[10, 14]\n",
      "trueeeeee\n",
      "(16, 16)\n",
      "[16, 3]\n",
      "trueeeeee\n",
      "(1, 16)\n",
      "[8, 15]\n",
      "trueeeeee\n",
      "(18, 16)\n",
      "[13, 18]\n",
      "trueeeeee\n",
      "(13, 17)\n",
      "[11, 12]\n",
      "trueeeeee\n",
      "(17, 19)\n",
      "[2, 8]\n",
      "trueeeeee\n",
      "(19, 1)\n",
      "[16, 16]\n",
      "trueeeeee\n",
      "(12, 11)\n",
      "[17, 18]\n",
      "trueeeeee\n",
      "(8, 5)\n",
      "[8, 17]\n",
      "trueeeeee\n",
      "(18, 0)\n",
      "[16, 17]\n",
      "trueeeeee\n",
      "(15, 10)\n",
      "[8, 6]\n",
      "trueeeeee\n",
      "(19, 0)\n",
      "[5, 16]\n",
      "trueeeeee\n",
      "(1, 16)\n",
      "[6, 12]\n",
      "trueeeeee\n",
      "(8, 15)\n",
      "[12, 8]\n",
      "trueeeeee\n",
      "(13, 12)\n",
      "[17, 10]\n",
      "trueeeeee\n",
      "(6, 8)\n",
      "[11, 0]\n",
      "trueeeeee\n",
      "(11, 10)\n",
      "[10, 13]\n",
      "trueeeeee\n",
      "(9, 8)\n",
      "[5, 0]\n",
      "trueeeeee\n",
      "(15, 17)\n",
      "[12, 9]\n",
      "trueeeeee\n",
      "(11, 10)\n",
      "[19, 14]\n",
      "trueeeeee\n",
      "(14, 9)\n",
      "[9, 15]\n",
      "trueeeeee\n",
      "(11, 6)\n",
      "[2, 13]\n",
      "trueeeeee\n",
      "(5, 4)\n",
      "[0, 4]\n",
      "trueeeeee\n",
      "(7, 2)\n",
      "[1, 10]\n",
      "trueeeeee\n",
      "(8, 12)\n",
      "[16, 6]\n",
      "trueeeeee\n",
      "(15, 9)\n",
      "[8, 18]\n",
      "trueeeeee\n",
      "(2, 7)\n",
      "[3, 4]\n",
      "trueeeeee\n",
      "(4, 8)\n",
      "[17, 11]\n",
      "trueeeeee\n",
      "(9, 8)\n",
      "[19, 7]\n",
      "trueeeeee\n",
      "(9, 10)\n",
      "[6, 3]\n",
      "trueeeeee\n",
      "(8, 1)\n",
      "[11, 17]\n",
      "trueeeeee\n",
      "(6, 16)\n",
      "[17, 13]\n",
      "trueeeeee\n",
      "(16, 10)\n",
      "[13, 10]\n",
      "trueeeeee\n",
      "(14, 12)\n",
      "[7, 6]\n",
      "trueeeeee\n",
      "(1, 14)\n",
      "[2, 18]\n",
      "trueeeeee\n",
      "(19, 9)\n",
      "[7, 19]\n",
      "trueeeeee\n",
      "(10, 16)\n",
      "[13, 9]\n",
      "trueeeeee\n",
      "(8, 2)\n",
      "[18, 6]\n",
      "trueeeeee\n",
      "(2, 3)\n",
      "[5, 11]\n",
      "trueeeeee\n",
      "(2, 3)\n",
      "[15, 0]\n",
      "trueeeeee\n",
      "(19, 4)\n",
      "[19, 8]\n",
      "trueeeeee\n",
      "(12, 7)\n",
      "[16, 14]\n",
      "trueeeeee\n",
      "(0, 11)\n",
      "[17, 8]\n",
      "trueeeeee\n",
      "(13, 7)\n",
      "[4, 18]\n",
      "trueeeeee\n",
      "(10, 2)\n",
      "[9, 14]\n",
      "trueeeeee\n",
      "(19, 18)\n",
      "[15, 11]\n",
      "trueeeeee\n",
      "(0, 8)\n",
      "[14, 7]\n",
      "trueeeeee\n",
      "(17, 16)\n",
      "[1, 17]\n",
      "trueeeeee\n",
      "(9, 19)\n",
      "[17, 4]\n",
      "trueeeeee\n",
      "(2, 8)\n",
      "[16, 19]\n",
      "trueeeeee\n",
      "(4, 5)\n",
      "[0, 19]\n",
      "trueeeeee\n",
      "(15, 14)\n",
      "[17, 6]\n",
      "trueeeeee\n",
      "(2, 1)\n",
      "[14, 18]\n",
      "trueeeeee\n",
      "(2, 9)\n",
      "[19, 1]\n",
      "trueeeeee\n",
      "(8, 16)\n",
      "[5, 19]\n",
      "trueeeeee\n",
      "(13, 13)\n",
      "[9, 9]\n",
      "trueeeeee\n",
      "(0, 5)\n",
      "[17, 10]\n",
      "trueeeeee\n",
      "(6, 18)\n",
      "[18, 19]\n",
      "trueeeeee\n",
      "(15, 7)\n",
      "[6, 2]\n",
      "trueeeeee\n",
      "(9, 10)\n",
      "[0, 9]\n",
      "trueeeeee\n",
      "(18, 9)\n",
      "[8, 4]\n",
      "trueeeeee\n",
      "(11, 17)\n",
      "[10, 17]\n",
      "trueeeeee\n",
      "(13, 12)\n",
      "[2, 14]\n",
      "trueeeeee\n",
      "(19, 8)\n",
      "[3, 11]\n",
      "trueeeeee\n",
      "(12, 3)\n",
      "[19, 7]\n",
      "trueeeeee\n",
      "(18, 10)\n",
      "[10, 17]\n",
      "trueeeeee\n",
      "(10, 15)\n",
      "[9, 12]\n",
      "trueeeeee\n",
      "(11, 2)\n",
      "[1, 0]\n",
      "trueeeeee\n",
      "(12, 12)\n",
      "[11, 13]\n",
      "trueeeeee\n",
      "(4, 2)\n",
      "[17, 14]\n",
      "trueeeeee\n",
      "(13, 17)\n",
      "[16, 8]\n",
      "trueeeeee\n",
      "(18, 9)\n",
      "[10, 13]\n",
      "trueeeeee\n",
      "(15, 18)\n",
      "[16, 2]\n",
      "trueeeeee\n",
      "(11, 5)\n",
      "[4, 15]\n",
      "trueeeeee\n",
      "(3, 1)\n",
      "[15, 8]\n",
      "trueeeeee\n",
      "(3, 17)\n",
      "[12, 14]\n",
      "trueeeeee\n",
      "(2, 0)\n",
      "[1, 0]\n",
      "trueeeeee\n",
      "(4, 12)\n",
      "[13, 10]\n",
      "trueeeeee\n",
      "(8, 5)\n",
      "[17, 13]\n",
      "trueeeeee\n",
      "(8, 14)\n",
      "[18, 0]\n",
      "trueeeeee\n",
      "(15, 3)\n",
      "[10, 2]\n",
      "trueeeeee\n",
      "(15, 2)\n",
      "[2, 17]\n",
      "trueeeeee\n",
      "(6, 16)\n",
      "[15, 19]\n",
      "trueeeeee\n",
      "(5, 18)\n",
      "[10, 12]\n",
      "trueeeeee\n",
      "(10, 2)\n",
      "[18, 1]\n",
      "trueeeeee\n",
      "(5, 2)\n",
      "[13, 7]\n",
      "trueeeeee\n",
      "(13, 0)\n",
      "[7, 14]\n",
      "trueeeeee\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_of_robots = 1\n",
    "success = 0\n",
    "actions = [[0,0],[-1,0],[0,1],[1,0],[0,-1]]\n",
    "# actions = [[0,0],[-1,0],[0,-1],[1,0],[0,1]]\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "with torch.inference_mode():\n",
    "    for case in eval_cases:\n",
    "        threshold = 0\n",
    "        for path in case['paths']:\n",
    "            threshold+= len(path)\n",
    "        threshold = 3*threshold # from the paper\n",
    "        num_of_steps = 0\n",
    "        robots_reached = set()\n",
    "        step_number = 0\n",
    "        current_nodes = []\n",
    "        path_robot_zero = []\n",
    "        while num_of_steps <= threshold and len(robots_reached) <=6:\n",
    "            paths = []\n",
    "            goals = []\n",
    "            for robot in range(num_of_robots):\n",
    "                if step_number == 0:\n",
    "                    current_node = case['paths'][robot][step_number]\n",
    "                    current_nodes.append(list(current_node))\n",
    "                    if robot == 0:\n",
    "                        print(current_node)\n",
    "                        print(list(case['paths'][robot][-1]))\n",
    "                    paths.append([list(current_node)])\n",
    "                    goals.append(list(case['goal_positions'][robot]))\n",
    "                else:\n",
    "                    paths.append([current_nodes[robot]])\n",
    "                    goals.append(list(case['goal_positions'][robot]))\n",
    "            # print(current_nodes[0])\n",
    "            step_case = {\"start_positions\": current_nodes, \"goal_positions\": goals,\"paths\":paths}\n",
    "            # print(paths)\n",
    "            # print(\"start_preprocessing\")\n",
    "            \n",
    "            p_new = Preprocessing(grid,[step_case],3)\n",
    "            data_tensors_new  = p_new.begin()\n",
    "            # if num_of_steps == 11:\n",
    "            #     # print(len(step_case['start_positions']))\n",
    "            #     print(step_case)\n",
    "            #     print(current_nodes[0])\n",
    "            #     print(data_tensors_new[0]['channel 2'])\n",
    "                \n",
    "            # print(\"start_encoding\")\n",
    "            # torch.manual_seed(42)\n",
    "            new_encoder = Encode(data_tensors_new,num_of_robots)\n",
    "            new_encoded_tensors = new_encoder.begin()\n",
    "            # print(new_encoded_tensors[0])\n",
    "            # print(\"start adj\")\n",
    "            new_adj = adj_mat([step_case],3)\n",
    "            new_adj_matrices = new_adj.get_adj_mat()\n",
    "            # print(\"start gnn\")\n",
    "            new_comm_gnn = Communication_GNN(encoded=new_encoded_tensors, adj_mat=new_adj_matrices, num_of_agents=num_of_robots)\n",
    "            new_gnn_features = new_comm_gnn.begin()\n",
    "            gnn_features_cat = torch.cat([new_gnn_features[key][subkey] for key in new_gnn_features for subkey in new_gnn_features[key]])\n",
    "            reshaped_gnn_features = []\n",
    "            temp = []\n",
    "            c = 1\n",
    "            for i in range(len(gnn_features_cat)):\n",
    "                temp.append(gnn_features_cat[i].tolist())\n",
    "                if c % num_of_robots == 0:\n",
    "                    reshaped_gnn_features.append(torch.Tensor(temp))\n",
    "                    temp = []\n",
    "                c+=1\n",
    "            # print(reshaped_gnn_features[0][0])\n",
    "            # print(\"start check\")\n",
    "            for robot in range(num_of_robots):\n",
    "                if robot not in robots_reached:\n",
    "                    # prediction = model.model(torch.tensor(reshaped_gnn_features[0][robot],dtype=torch.float32).unsqueeze(0).to(device))\n",
    "                    prediction = model.model(torch.tensor(reshaped_gnn_features[0][robot],dtype=torch.float32).unsqueeze(0).to(device))\n",
    "                    # print(prediction)\n",
    "                    predicted_class = torch.argmax(prediction, dim=1)\n",
    "                    right_state = False\n",
    "                    if robot == 0 and robot not in robots_reached:\n",
    "                        # print(current_nodes[robot])\n",
    "                        # print(predicted_class)\n",
    "                        if (current_nodes[robot][0]+1 == goals[robot][0]) and (current_nodes[robot][1] == goals[robot][1]):\n",
    "                            predicted_class = 3\n",
    "                            right_state = True\n",
    "                        path_robot_zero.append(predicted_class)\n",
    "                        next_r = [current_nodes[robot][0] + actions[predicted_class][0],current_nodes[robot][1] + actions[predicted_class][1]]\n",
    "                        # print(next_r)\n",
    "                    next = [current_nodes[robot][0] + actions[predicted_class][0],current_nodes[robot][1] + actions[predicted_class][1]]\n",
    "                    if next[0]>=20 or next[0]<0 or next[1]>=20 or next[1]<0:\n",
    "                        # current_nodes[robot] = [current_nodes[robot][0] + actions[0][0],current_nodes[robot][1] + actions[0][1]]\n",
    "                        pass\n",
    "                    else:\n",
    "                        current_nodes[robot] = [current_nodes[robot][0] + actions[predicted_class][0],current_nodes[robot][1] + actions[predicted_class][1]]\n",
    "                    if (current_nodes[robot][0] == goals[robot][0]) and (current_nodes[robot][1] == goals[robot][1]):\n",
    "                        robots_reached.add(robot)\n",
    "                        print(\"trueeeeee\")\n",
    "            step_number = 1\n",
    "            num_of_steps+=1\n",
    "        if len(robots_reached) == num_of_robots:\n",
    "            success += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(success)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
