{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for Decentralized Multi-Robot Path Planning In Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset_Generator import DatasetGenerator \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [[0 for _ in range(20)] for _ in range(20)]  # Generate grid map\n",
    "dataset_generator = DatasetGenerator(num_cases=5000, num_agents=6, grid=grid)\n",
    "# cases = dataset_generator.generate_cases()\n",
    "# dataset_generator.save_cases_to_file(cases, \"dataset.json\")\n",
    "# print(f\"Generated and saved {len(cases)} cases.\")\n",
    "cases = dataset_generator.load_cases_from_file(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pre_processing import Preprocessing\n",
    "p = Preprocessing(grid,cases,3)\n",
    "data_tensors  = p.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data_tensors.txt\", 'w') as file:\n",
    "#     file.write(str(data_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoder import Encode\n",
    "encoder = Encode(data_tensors,6)\n",
    "encoded_tensors = encoder.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"encoded_tensors.txt\", 'w') as file:\n",
    "#     file.write(str(encoded_tensors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Adjacency_Matrix import adj_mat\n",
    "adj = adj_mat(cases,3)\n",
    "adj_matrices = adj.get_adj_mat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"Adj_Matrices.txt\", 'w') as file:\n",
    "#     file.write(str(adj_matrices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\anaconda3\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "from GNN_file import Communication_GNN\n",
    "comm_gnn = Communication_GNN(encoded=encoded_tensors, adj_mat=adj_matrices, num_of_agents=6)\n",
    "gnn_features = comm_gnn.begin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"GNN_Features.txt\", 'w') as file:\n",
    "#     file.write(str(gnn_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Extarct_Actions import Action_Extractor\n",
    "action_extractor = Action_Extractor(cases, 6)\n",
    "actions = action_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_features = torch.cat([gnn_features[key][subkey] for key in gnn_features for subkey in gnn_features[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.tensor([actions[key][subkey] for key in actions for subkey in actions[key]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_gnn_features = []\n",
    "temp = []\n",
    "c = 1\n",
    "for i in range(len(gnn_features)):\n",
    "    temp.append(gnn_features[i].tolist())\n",
    "    if c % 6 == 0:\n",
    "        reshaped_gnn_features.append(torch.Tensor(temp))\n",
    "        temp = []\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(len(actions)):\n",
    "    dataset.append((reshaped_gnn_features[i],actions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLP_Action import ActionMLP\n",
    "mlp = ActionMLP(input_dim= 128, hidden_dim= 128, num_actions= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_Generation import Generate_Model\n",
    "model = Generate_Model(model = mlp, dataset=dataset,num_epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] - Train Loss: 1.1662 - LR: 0.001000 - Val Loss: 1.1284 - Val Acc: 77.30%\n",
      "Epoch [2/300] - Train Loss: 1.1331 - LR: 0.001000 - Val Loss: 1.1214 - Val Acc: 77.97%\n",
      "Epoch [3/300] - Train Loss: 1.1264 - LR: 0.001000 - Val Loss: 1.1179 - Val Acc: 78.35%\n",
      "Epoch [4/300] - Train Loss: 1.1232 - LR: 0.001000 - Val Loss: 1.1131 - Val Acc: 78.78%\n",
      "Epoch [5/300] - Train Loss: 1.1205 - LR: 0.001000 - Val Loss: 1.1103 - Val Acc: 79.07%\n",
      "Epoch [6/300] - Train Loss: 1.1185 - LR: 0.001000 - Val Loss: 1.1086 - Val Acc: 79.25%\n",
      "Epoch [7/300] - Train Loss: 1.1164 - LR: 0.001000 - Val Loss: 1.1082 - Val Acc: 79.28%\n",
      "Epoch [8/300] - Train Loss: 1.1152 - LR: 0.001000 - Val Loss: 1.1052 - Val Acc: 79.55%\n",
      "Epoch [9/300] - Train Loss: 1.1137 - LR: 0.001000 - Val Loss: 1.1059 - Val Acc: 79.53%\n",
      "Epoch [10/300] - Train Loss: 1.1128 - LR: 0.001000 - Val Loss: 1.1045 - Val Acc: 79.64%\n",
      "Epoch [11/300] - Train Loss: 1.1118 - LR: 0.001000 - Val Loss: 1.1039 - Val Acc: 79.72%\n",
      "Epoch [12/300] - Train Loss: 1.1111 - LR: 0.001000 - Val Loss: 1.1020 - Val Acc: 79.89%\n",
      "Epoch [13/300] - Train Loss: 1.1101 - LR: 0.001000 - Val Loss: 1.1031 - Val Acc: 79.79%\n",
      "Epoch [14/300] - Train Loss: 1.1097 - LR: 0.001000 - Val Loss: 1.1015 - Val Acc: 79.95%\n",
      "Epoch [15/300] - Train Loss: 1.1092 - LR: 0.001000 - Val Loss: 1.1020 - Val Acc: 79.86%\n",
      "Epoch [16/300] - Train Loss: 1.1090 - LR: 0.001000 - Val Loss: 1.1015 - Val Acc: 79.95%\n",
      "Epoch [17/300] - Train Loss: 1.1083 - LR: 0.001000 - Val Loss: 1.1018 - Val Acc: 79.86%\n",
      "Epoch [18/300] - Train Loss: 1.1073 - LR: 0.001000 - Val Loss: 1.1001 - Val Acc: 80.09%\n",
      "Epoch [19/300] - Train Loss: 1.1071 - LR: 0.001000 - Val Loss: 1.1009 - Val Acc: 79.97%\n",
      "Epoch [20/300] - Train Loss: 1.1068 - LR: 0.001000 - Val Loss: 1.1008 - Val Acc: 79.97%\n",
      "Epoch [21/300] - Train Loss: 1.1063 - LR: 0.001000 - Val Loss: 1.1002 - Val Acc: 80.07%\n",
      "Epoch [22/300] - Train Loss: 1.1064 - LR: 0.001000 - Val Loss: 1.0999 - Val Acc: 80.12%\n",
      "Epoch [23/300] - Train Loss: 1.1056 - LR: 0.001000 - Val Loss: 1.0990 - Val Acc: 80.21%\n",
      "Epoch [24/300] - Train Loss: 1.1055 - LR: 0.001000 - Val Loss: 1.0979 - Val Acc: 80.34%\n",
      "Epoch [25/300] - Train Loss: 1.1052 - LR: 0.001000 - Val Loss: 1.0983 - Val Acc: 80.23%\n",
      "Epoch [26/300] - Train Loss: 1.1050 - LR: 0.001000 - Val Loss: 1.0993 - Val Acc: 80.18%\n",
      "Epoch [27/300] - Train Loss: 1.1045 - LR: 0.001000 - Val Loss: 1.0986 - Val Acc: 80.25%\n",
      "Epoch [28/300] - Train Loss: 1.1040 - LR: 0.001000 - Val Loss: 1.0985 - Val Acc: 80.25%\n",
      "Epoch [29/300] - Train Loss: 1.1041 - LR: 0.001000 - Val Loss: 1.0986 - Val Acc: 80.25%\n",
      "Epoch [30/300] - Train Loss: 1.1036 - LR: 0.001000 - Val Loss: 1.0976 - Val Acc: 80.32%\n",
      "Epoch [31/300] - Train Loss: 1.1036 - LR: 0.001000 - Val Loss: 1.0975 - Val Acc: 80.36%\n",
      "Epoch [32/300] - Train Loss: 1.1034 - LR: 0.001000 - Val Loss: 1.0970 - Val Acc: 80.40%\n",
      "Epoch [33/300] - Train Loss: 1.1030 - LR: 0.001000 - Val Loss: 1.0977 - Val Acc: 80.31%\n",
      "Epoch [34/300] - Train Loss: 1.1027 - LR: 0.001000 - Val Loss: 1.0974 - Val Acc: 80.31%\n",
      "Epoch [35/300] - Train Loss: 1.1029 - LR: 0.001000 - Val Loss: 1.0965 - Val Acc: 80.41%\n",
      "Epoch [36/300] - Train Loss: 1.1025 - LR: 0.001000 - Val Loss: 1.0959 - Val Acc: 80.49%\n",
      "Epoch [37/300] - Train Loss: 1.1024 - LR: 0.001000 - Val Loss: 1.0966 - Val Acc: 80.47%\n",
      "Epoch [38/300] - Train Loss: 1.1022 - LR: 0.001000 - Val Loss: 1.0969 - Val Acc: 80.43%\n",
      "Epoch [39/300] - Train Loss: 1.1024 - LR: 0.001000 - Val Loss: 1.0986 - Val Acc: 80.22%\n",
      "Epoch [40/300] - Train Loss: 1.1018 - LR: 0.001000 - Val Loss: 1.0964 - Val Acc: 80.45%\n",
      "Epoch [41/300] - Train Loss: 1.1016 - LR: 0.001000 - Val Loss: 1.0967 - Val Acc: 80.40%\n",
      "Epoch [42/300] - Train Loss: 1.1017 - LR: 0.001000 - Val Loss: 1.0964 - Val Acc: 80.42%\n",
      "Epoch [43/300] - Train Loss: 1.1013 - LR: 0.001000 - Val Loss: 1.0961 - Val Acc: 80.50%\n",
      "Epoch [44/300] - Train Loss: 1.1012 - LR: 0.001000 - Val Loss: 1.0962 - Val Acc: 80.42%\n",
      "Epoch [45/300] - Train Loss: 1.1012 - LR: 0.001000 - Val Loss: 1.0957 - Val Acc: 80.55%\n",
      "Epoch [46/300] - Train Loss: 1.1009 - LR: 0.001000 - Val Loss: 1.0961 - Val Acc: 80.47%\n",
      "Epoch [47/300] - Train Loss: 1.1009 - LR: 0.001000 - Val Loss: 1.0959 - Val Acc: 80.53%\n",
      "Epoch [48/300] - Train Loss: 1.1003 - LR: 0.001000 - Val Loss: 1.0960 - Val Acc: 80.44%\n",
      "Epoch [49/300] - Train Loss: 1.1003 - LR: 0.001000 - Val Loss: 1.0953 - Val Acc: 80.54%\n",
      "Epoch [50/300] - Train Loss: 1.1004 - LR: 0.001000 - Val Loss: 1.0958 - Val Acc: 80.47%\n",
      "Epoch [51/300] - Train Loss: 1.1011 - LR: 0.001000 - Val Loss: 1.0958 - Val Acc: 80.53%\n",
      "Epoch [52/300] - Train Loss: 1.1003 - LR: 0.001000 - Val Loss: 1.0947 - Val Acc: 80.62%\n",
      "Epoch [53/300] - Train Loss: 1.1002 - LR: 0.001000 - Val Loss: 1.0965 - Val Acc: 80.43%\n",
      "Epoch [54/300] - Train Loss: 1.0999 - LR: 0.001000 - Val Loss: 1.0959 - Val Acc: 80.48%\n",
      "Epoch [55/300] - Train Loss: 1.0998 - LR: 0.001000 - Val Loss: 1.0960 - Val Acc: 80.41%\n",
      "Epoch [56/300] - Train Loss: 1.0997 - LR: 0.001000 - Val Loss: 1.0952 - Val Acc: 80.57%\n",
      "Epoch [57/300] - Train Loss: 1.0999 - LR: 0.001000 - Val Loss: 1.0960 - Val Acc: 80.48%\n",
      "Epoch [58/300] - Train Loss: 1.0993 - LR: 0.001000 - Val Loss: 1.0957 - Val Acc: 80.52%\n",
      "Epoch [59/300] - Train Loss: 1.0998 - LR: 0.001000 - Val Loss: 1.0955 - Val Acc: 80.54%\n",
      "Epoch [60/300] - Train Loss: 1.0998 - LR: 0.001000 - Val Loss: 1.0945 - Val Acc: 80.65%\n",
      "Epoch [61/300] - Train Loss: 1.0991 - LR: 0.001000 - Val Loss: 1.0963 - Val Acc: 80.48%\n",
      "Epoch [62/300] - Train Loss: 1.0991 - LR: 0.001000 - Val Loss: 1.0954 - Val Acc: 80.54%\n",
      "Epoch [63/300] - Train Loss: 1.0990 - LR: 0.001000 - Val Loss: 1.0945 - Val Acc: 80.66%\n",
      "Epoch [64/300] - Train Loss: 1.0990 - LR: 0.001000 - Val Loss: 1.0959 - Val Acc: 80.46%\n",
      "Epoch [65/300] - Train Loss: 1.0987 - LR: 0.001000 - Val Loss: 1.0947 - Val Acc: 80.61%\n",
      "Epoch [66/300] - Train Loss: 1.0989 - LR: 0.001000 - Val Loss: 1.0948 - Val Acc: 80.60%\n",
      "Epoch [67/300] - Train Loss: 1.0987 - LR: 0.001000 - Val Loss: 1.0948 - Val Acc: 80.63%\n",
      "Epoch [68/300] - Train Loss: 1.0986 - LR: 0.001000 - Val Loss: 1.0942 - Val Acc: 80.68%\n",
      "Epoch [69/300] - Train Loss: 1.0984 - LR: 0.001000 - Val Loss: 1.0948 - Val Acc: 80.57%\n",
      "Epoch [70/300] - Train Loss: 1.0982 - LR: 0.001000 - Val Loss: 1.0951 - Val Acc: 80.53%\n",
      "Epoch [71/300] - Train Loss: 1.0985 - LR: 0.001000 - Val Loss: 1.0956 - Val Acc: 80.52%\n",
      "Epoch [72/300] - Train Loss: 1.0985 - LR: 0.001000 - Val Loss: 1.0940 - Val Acc: 80.62%\n",
      "Epoch [73/300] - Train Loss: 1.0982 - LR: 0.001000 - Val Loss: 1.0938 - Val Acc: 80.66%\n",
      "Epoch [74/300] - Train Loss: 1.0983 - LR: 0.001000 - Val Loss: 1.0946 - Val Acc: 80.64%\n",
      "Epoch [75/300] - Train Loss: 1.0982 - LR: 0.001000 - Val Loss: 1.0944 - Val Acc: 80.64%\n",
      "Epoch [76/300] - Train Loss: 1.0985 - LR: 0.001000 - Val Loss: 1.0948 - Val Acc: 80.59%\n",
      "Epoch [77/300] - Train Loss: 1.0978 - LR: 0.001000 - Val Loss: 1.0939 - Val Acc: 80.73%\n",
      "Epoch [78/300] - Train Loss: 1.0979 - LR: 0.001000 - Val Loss: 1.0943 - Val Acc: 80.65%\n",
      "Epoch [79/300] - Train Loss: 1.0979 - LR: 0.001000 - Val Loss: 1.0946 - Val Acc: 80.64%\n",
      "Epoch [80/300] - Train Loss: 1.0974 - LR: 0.001000 - Val Loss: 1.0936 - Val Acc: 80.75%\n",
      "Epoch [81/300] - Train Loss: 1.0976 - LR: 0.001000 - Val Loss: 1.0941 - Val Acc: 80.69%\n",
      "Epoch [82/300] - Train Loss: 1.0977 - LR: 0.001000 - Val Loss: 1.0949 - Val Acc: 80.60%\n",
      "Epoch [83/300] - Train Loss: 1.0977 - LR: 0.001000 - Val Loss: 1.0933 - Val Acc: 80.73%\n",
      "Epoch [84/300] - Train Loss: 1.0974 - LR: 0.001000 - Val Loss: 1.0934 - Val Acc: 80.76%\n",
      "Epoch [85/300] - Train Loss: 1.0972 - LR: 0.001000 - Val Loss: 1.0942 - Val Acc: 80.69%\n",
      "Epoch [86/300] - Train Loss: 1.0973 - LR: 0.001000 - Val Loss: 1.0944 - Val Acc: 80.63%\n",
      "Epoch [87/300] - Train Loss: 1.0977 - LR: 0.001000 - Val Loss: 1.0936 - Val Acc: 80.70%\n",
      "Epoch [88/300] - Train Loss: 1.0971 - LR: 0.001000 - Val Loss: 1.0936 - Val Acc: 80.73%\n",
      "Epoch [89/300] - Train Loss: 1.0970 - LR: 0.001000 - Val Loss: 1.0943 - Val Acc: 80.64%\n",
      "Epoch [90/300] - Train Loss: 1.0970 - LR: 0.001000 - Val Loss: 1.0939 - Val Acc: 80.68%\n",
      "Epoch [91/300] - Train Loss: 1.0974 - LR: 0.001000 - Val Loss: 1.0941 - Val Acc: 80.68%\n",
      "Epoch [92/300] - Train Loss: 1.0970 - LR: 0.001000 - Val Loss: 1.0937 - Val Acc: 80.71%\n",
      "Epoch [93/300] - Train Loss: 1.0970 - LR: 0.001000 - Val Loss: 1.0936 - Val Acc: 80.74%\n",
      "Epoch [94/300] - Train Loss: 1.0974 - LR: 0.000500 - Val Loss: 1.0942 - Val Acc: 80.66%\n",
      "Epoch [95/300] - Train Loss: 1.0948 - LR: 0.000500 - Val Loss: 1.0923 - Val Acc: 80.88%\n",
      "Epoch [96/300] - Train Loss: 1.0948 - LR: 0.000500 - Val Loss: 1.0921 - Val Acc: 80.91%\n",
      "Epoch [97/300] - Train Loss: 1.0946 - LR: 0.000500 - Val Loss: 1.0924 - Val Acc: 80.88%\n",
      "Epoch [98/300] - Train Loss: 1.0946 - LR: 0.000500 - Val Loss: 1.0923 - Val Acc: 80.89%\n",
      "Epoch [99/300] - Train Loss: 1.0940 - LR: 0.000500 - Val Loss: 1.0924 - Val Acc: 80.87%\n",
      "Epoch [100/300] - Train Loss: 1.0938 - LR: 0.000500 - Val Loss: 1.0917 - Val Acc: 80.94%\n",
      "Epoch [101/300] - Train Loss: 1.0940 - LR: 0.000500 - Val Loss: 1.0918 - Val Acc: 80.91%\n",
      "Epoch [102/300] - Train Loss: 1.0938 - LR: 0.000500 - Val Loss: 1.0920 - Val Acc: 80.91%\n",
      "Epoch [103/300] - Train Loss: 1.0939 - LR: 0.000500 - Val Loss: 1.0917 - Val Acc: 80.94%\n",
      "Epoch [104/300] - Train Loss: 1.0940 - LR: 0.000500 - Val Loss: 1.0920 - Val Acc: 80.95%\n",
      "Epoch [105/300] - Train Loss: 1.0936 - LR: 0.000500 - Val Loss: 1.0919 - Val Acc: 80.95%\n",
      "Epoch [106/300] - Train Loss: 1.0937 - LR: 0.000500 - Val Loss: 1.0919 - Val Acc: 80.93%\n",
      "Epoch [107/300] - Train Loss: 1.0941 - LR: 0.000500 - Val Loss: 1.0915 - Val Acc: 80.94%\n",
      "Epoch [108/300] - Train Loss: 1.0937 - LR: 0.000500 - Val Loss: 1.0919 - Val Acc: 80.96%\n",
      "Epoch [109/300] - Train Loss: 1.0939 - LR: 0.000500 - Val Loss: 1.0914 - Val Acc: 81.00%\n",
      "Epoch [110/300] - Train Loss: 1.0932 - LR: 0.000500 - Val Loss: 1.0918 - Val Acc: 80.88%\n",
      "Epoch [111/300] - Train Loss: 1.0935 - LR: 0.000500 - Val Loss: 1.0915 - Val Acc: 80.95%\n",
      "Epoch [112/300] - Train Loss: 1.0935 - LR: 0.000500 - Val Loss: 1.0919 - Val Acc: 80.90%\n",
      "Epoch [113/300] - Train Loss: 1.0932 - LR: 0.000500 - Val Loss: 1.0919 - Val Acc: 80.97%\n",
      "Epoch [114/300] - Train Loss: 1.0936 - LR: 0.000500 - Val Loss: 1.0919 - Val Acc: 80.92%\n",
      "Epoch [115/300] - Train Loss: 1.0936 - LR: 0.000500 - Val Loss: 1.0918 - Val Acc: 80.90%\n",
      "Epoch [116/300] - Train Loss: 1.0937 - LR: 0.000500 - Val Loss: 1.0919 - Val Acc: 80.93%\n",
      "Epoch [117/300] - Train Loss: 1.0934 - LR: 0.000500 - Val Loss: 1.0923 - Val Acc: 80.84%\n",
      "Epoch [118/300] - Train Loss: 1.0934 - LR: 0.000250 - Val Loss: 1.0917 - Val Acc: 80.95%\n",
      "Epoch [119/300] - Train Loss: 1.0927 - LR: 0.000250 - Val Loss: 1.0914 - Val Acc: 80.96%\n",
      "Epoch [120/300] - Train Loss: 1.0922 - LR: 0.000250 - Val Loss: 1.0916 - Val Acc: 80.92%\n",
      "Epoch [121/300] - Train Loss: 1.0921 - LR: 0.000250 - Val Loss: 1.0914 - Val Acc: 80.96%\n",
      "Epoch [122/300] - Train Loss: 1.0921 - LR: 0.000250 - Val Loss: 1.0913 - Val Acc: 80.96%\n",
      "Epoch [123/300] - Train Loss: 1.0920 - LR: 0.000250 - Val Loss: 1.0912 - Val Acc: 80.97%\n",
      "Epoch [124/300] - Train Loss: 1.0919 - LR: 0.000250 - Val Loss: 1.0911 - Val Acc: 81.00%\n",
      "Epoch [125/300] - Train Loss: 1.0920 - LR: 0.000250 - Val Loss: 1.0912 - Val Acc: 80.99%\n",
      "Epoch [126/300] - Train Loss: 1.0918 - LR: 0.000250 - Val Loss: 1.0911 - Val Acc: 81.00%\n",
      "Epoch [127/300] - Train Loss: 1.0917 - LR: 0.000250 - Val Loss: 1.0911 - Val Acc: 80.97%\n",
      "Epoch [128/300] - Train Loss: 1.0918 - LR: 0.000250 - Val Loss: 1.0910 - Val Acc: 80.99%\n",
      "Epoch [129/300] - Train Loss: 1.0920 - LR: 0.000250 - Val Loss: 1.0913 - Val Acc: 80.98%\n",
      "Epoch [130/300] - Train Loss: 1.0918 - LR: 0.000250 - Val Loss: 1.0910 - Val Acc: 81.00%\n",
      "Epoch [131/300] - Train Loss: 1.0915 - LR: 0.000250 - Val Loss: 1.0912 - Val Acc: 80.97%\n",
      "Epoch [132/300] - Train Loss: 1.0918 - LR: 0.000250 - Val Loss: 1.0912 - Val Acc: 80.99%\n",
      "Epoch [133/300] - Train Loss: 1.0915 - LR: 0.000250 - Val Loss: 1.0912 - Val Acc: 80.96%\n",
      "Epoch [134/300] - Train Loss: 1.0915 - LR: 0.000250 - Val Loss: 1.0912 - Val Acc: 81.00%\n",
      "Epoch [135/300] - Train Loss: 1.0918 - LR: 0.000125 - Val Loss: 1.0911 - Val Acc: 80.99%\n",
      "Epoch [136/300] - Train Loss: 1.0911 - LR: 0.000125 - Val Loss: 1.0909 - Val Acc: 81.03%\n",
      "Epoch [137/300] - Train Loss: 1.0909 - LR: 0.000125 - Val Loss: 1.0908 - Val Acc: 81.03%\n",
      "Epoch [138/300] - Train Loss: 1.0911 - LR: 0.000125 - Val Loss: 1.0909 - Val Acc: 81.02%\n",
      "Epoch [139/300] - Train Loss: 1.0914 - LR: 0.000125 - Val Loss: 1.0908 - Val Acc: 81.06%\n",
      "Epoch [140/300] - Train Loss: 1.0908 - LR: 0.000125 - Val Loss: 1.0907 - Val Acc: 81.06%\n",
      "Epoch [141/300] - Train Loss: 1.0909 - LR: 0.000125 - Val Loss: 1.0908 - Val Acc: 81.05%\n",
      "Epoch [142/300] - Train Loss: 1.0908 - LR: 0.000125 - Val Loss: 1.0907 - Val Acc: 81.05%\n",
      "Epoch [143/300] - Train Loss: 1.0908 - LR: 0.000125 - Val Loss: 1.0907 - Val Acc: 81.07%\n",
      "Epoch [144/300] - Train Loss: 1.0909 - LR: 0.000125 - Val Loss: 1.0910 - Val Acc: 81.01%\n",
      "Epoch [145/300] - Train Loss: 1.0906 - LR: 0.000125 - Val Loss: 1.0908 - Val Acc: 81.01%\n",
      "Epoch [146/300] - Train Loss: 1.0910 - LR: 0.000125 - Val Loss: 1.0911 - Val Acc: 81.03%\n",
      "Epoch [147/300] - Train Loss: 1.0910 - LR: 0.000125 - Val Loss: 1.0909 - Val Acc: 81.00%\n",
      "Epoch [148/300] - Train Loss: 1.0908 - LR: 0.000125 - Val Loss: 1.0910 - Val Acc: 81.01%\n",
      "Epoch [149/300] - Train Loss: 1.0906 - LR: 0.000125 - Val Loss: 1.0907 - Val Acc: 81.04%\n",
      "Epoch [150/300] - Train Loss: 1.0906 - LR: 0.000063 - Val Loss: 1.0909 - Val Acc: 81.00%\n",
      "Epoch [151/300] - Train Loss: 1.0908 - LR: 0.000063 - Val Loss: 1.0908 - Val Acc: 81.03%\n",
      "Epoch [152/300] - Train Loss: 1.0902 - LR: 0.000063 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [153/300] - Train Loss: 1.0903 - LR: 0.000063 - Val Loss: 1.0907 - Val Acc: 81.04%\n",
      "Epoch [154/300] - Train Loss: 1.0906 - LR: 0.000063 - Val Loss: 1.0907 - Val Acc: 80.98%\n",
      "Epoch [155/300] - Train Loss: 1.0904 - LR: 0.000063 - Val Loss: 1.0908 - Val Acc: 80.99%\n",
      "Epoch [156/300] - Train Loss: 1.0906 - LR: 0.000063 - Val Loss: 1.0907 - Val Acc: 81.04%\n",
      "Epoch [157/300] - Train Loss: 1.0903 - LR: 0.000063 - Val Loss: 1.0906 - Val Acc: 81.03%\n",
      "Epoch [158/300] - Train Loss: 1.0907 - LR: 0.000063 - Val Loss: 1.0907 - Val Acc: 81.06%\n",
      "Epoch [159/300] - Train Loss: 1.0902 - LR: 0.000063 - Val Loss: 1.0908 - Val Acc: 81.01%\n",
      "Epoch [160/300] - Train Loss: 1.0902 - LR: 0.000063 - Val Loss: 1.0907 - Val Acc: 81.05%\n",
      "Epoch [161/300] - Train Loss: 1.0903 - LR: 0.000063 - Val Loss: 1.0906 - Val Acc: 81.07%\n",
      "Epoch [162/300] - Train Loss: 1.0901 - LR: 0.000063 - Val Loss: 1.0909 - Val Acc: 81.01%\n",
      "Epoch [163/300] - Train Loss: 1.0904 - LR: 0.000031 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [164/300] - Train Loss: 1.0903 - LR: 0.000031 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [165/300] - Train Loss: 1.0905 - LR: 0.000031 - Val Loss: 1.0910 - Val Acc: 80.99%\n",
      "Epoch [166/300] - Train Loss: 1.0896 - LR: 0.000031 - Val Loss: 1.0907 - Val Acc: 81.06%\n",
      "Epoch [167/300] - Train Loss: 1.0901 - LR: 0.000031 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [168/300] - Train Loss: 1.0900 - LR: 0.000031 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [169/300] - Train Loss: 1.0905 - LR: 0.000031 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [170/300] - Train Loss: 1.0901 - LR: 0.000031 - Val Loss: 1.0907 - Val Acc: 81.04%\n",
      "Epoch [171/300] - Train Loss: 1.0900 - LR: 0.000031 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [172/300] - Train Loss: 1.0904 - LR: 0.000031 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [173/300] - Train Loss: 1.0899 - LR: 0.000031 - Val Loss: 1.0907 - Val Acc: 81.04%\n",
      "Epoch [174/300] - Train Loss: 1.0898 - LR: 0.000031 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [175/300] - Train Loss: 1.0903 - LR: 0.000031 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [176/300] - Train Loss: 1.0902 - LR: 0.000031 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [177/300] - Train Loss: 1.0900 - LR: 0.000031 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [178/300] - Train Loss: 1.0900 - LR: 0.000031 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [179/300] - Train Loss: 1.0898 - LR: 0.000016 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [180/300] - Train Loss: 1.0900 - LR: 0.000016 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [181/300] - Train Loss: 1.0901 - LR: 0.000016 - Val Loss: 1.0909 - Val Acc: 81.03%\n",
      "Epoch [182/300] - Train Loss: 1.0901 - LR: 0.000016 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [183/300] - Train Loss: 1.0904 - LR: 0.000016 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [184/300] - Train Loss: 1.0902 - LR: 0.000016 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [185/300] - Train Loss: 1.0906 - LR: 0.000016 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [186/300] - Train Loss: 1.0905 - LR: 0.000016 - Val Loss: 1.0905 - Val Acc: 81.04%\n",
      "Epoch [187/300] - Train Loss: 1.0900 - LR: 0.000016 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [188/300] - Train Loss: 1.0900 - LR: 0.000016 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [189/300] - Train Loss: 1.0899 - LR: 0.000016 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [190/300] - Train Loss: 1.0900 - LR: 0.000008 - Val Loss: 1.0907 - Val Acc: 81.07%\n",
      "Epoch [191/300] - Train Loss: 1.0897 - LR: 0.000008 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [192/300] - Train Loss: 1.0899 - LR: 0.000008 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [193/300] - Train Loss: 1.0901 - LR: 0.000008 - Val Loss: 1.0907 - Val Acc: 81.02%\n",
      "Epoch [194/300] - Train Loss: 1.0901 - LR: 0.000008 - Val Loss: 1.0904 - Val Acc: 81.06%\n",
      "Epoch [195/300] - Train Loss: 1.0901 - LR: 0.000008 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [196/300] - Train Loss: 1.0897 - LR: 0.000008 - Val Loss: 1.0906 - Val Acc: 81.07%\n",
      "Epoch [197/300] - Train Loss: 1.0898 - LR: 0.000008 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [198/300] - Train Loss: 1.0903 - LR: 0.000008 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [199/300] - Train Loss: 1.0898 - LR: 0.000008 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [200/300] - Train Loss: 1.0903 - LR: 0.000008 - Val Loss: 1.0907 - Val Acc: 81.01%\n",
      "Epoch [201/300] - Train Loss: 1.0897 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [202/300] - Train Loss: 1.0904 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [203/300] - Train Loss: 1.0902 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [204/300] - Train Loss: 1.0902 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [205/300] - Train Loss: 1.0896 - LR: 0.000004 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [206/300] - Train Loss: 1.0899 - LR: 0.000004 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [207/300] - Train Loss: 1.0899 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.03%\n",
      "Epoch [208/300] - Train Loss: 1.0900 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.03%\n",
      "Epoch [209/300] - Train Loss: 1.0901 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [210/300] - Train Loss: 1.0898 - LR: 0.000004 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [211/300] - Train Loss: 1.0897 - LR: 0.000004 - Val Loss: 1.0906 - Val Acc: 81.02%\n",
      "Epoch [212/300] - Train Loss: 1.0901 - LR: 0.000002 - Val Loss: 1.0908 - Val Acc: 81.01%\n",
      "Epoch [213/300] - Train Loss: 1.0899 - LR: 0.000002 - Val Loss: 1.0907 - Val Acc: 81.05%\n",
      "Epoch [214/300] - Train Loss: 1.0901 - LR: 0.000002 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [215/300] - Train Loss: 1.0899 - LR: 0.000002 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [216/300] - Train Loss: 1.0895 - LR: 0.000002 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [217/300] - Train Loss: 1.0902 - LR: 0.000002 - Val Loss: 1.0907 - Val Acc: 81.05%\n",
      "Epoch [218/300] - Train Loss: 1.0898 - LR: 0.000002 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [219/300] - Train Loss: 1.0896 - LR: 0.000002 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [220/300] - Train Loss: 1.0899 - LR: 0.000002 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [221/300] - Train Loss: 1.0899 - LR: 0.000002 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [222/300] - Train Loss: 1.0897 - LR: 0.000002 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [223/300] - Train Loss: 1.0900 - LR: 0.000001 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [224/300] - Train Loss: 1.0900 - LR: 0.000001 - Val Loss: 1.0905 - Val Acc: 81.04%\n",
      "Epoch [225/300] - Train Loss: 1.0899 - LR: 0.000001 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [226/300] - Train Loss: 1.0897 - LR: 0.000001 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [227/300] - Train Loss: 1.0898 - LR: 0.000001 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [228/300] - Train Loss: 1.0900 - LR: 0.000001 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [229/300] - Train Loss: 1.0898 - LR: 0.000001 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [230/300] - Train Loss: 1.0903 - LR: 0.000001 - Val Loss: 1.0905 - Val Acc: 81.09%\n",
      "Epoch [231/300] - Train Loss: 1.0896 - LR: 0.000001 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [232/300] - Train Loss: 1.0901 - LR: 0.000001 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [233/300] - Train Loss: 1.0895 - LR: 0.000001 - Val Loss: 1.0906 - Val Acc: 81.07%\n",
      "Epoch [234/300] - Train Loss: 1.0896 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [235/300] - Train Loss: 1.0901 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [236/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [237/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [238/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0908 - Val Acc: 81.04%\n",
      "Epoch [239/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [240/300] - Train Loss: 1.0895 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [241/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [242/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [243/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [244/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.03%\n",
      "Epoch [245/300] - Train Loss: 1.0901 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [246/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0908 - Val Acc: 81.01%\n",
      "Epoch [247/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [248/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.07%\n",
      "Epoch [249/300] - Train Loss: 1.0896 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [250/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [251/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.01%\n",
      "Epoch [252/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [253/300] - Train Loss: 1.0896 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [254/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.04%\n",
      "Epoch [255/300] - Train Loss: 1.0901 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.03%\n",
      "Epoch [256/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [257/300] - Train Loss: 1.0901 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.01%\n",
      "Epoch [258/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [259/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.07%\n",
      "Epoch [260/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.05%\n",
      "Epoch [261/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.03%\n",
      "Epoch [262/300] - Train Loss: 1.0902 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.02%\n",
      "Epoch [263/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [264/300] - Train Loss: 1.0901 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [265/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [266/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.08%\n",
      "Epoch [267/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.03%\n",
      "Epoch [268/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [269/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [270/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.01%\n",
      "Epoch [271/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.04%\n",
      "Epoch [272/300] - Train Loss: 1.0895 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [273/300] - Train Loss: 1.0901 - LR: 0.000000 - Val Loss: 1.0904 - Val Acc: 81.05%\n",
      "Epoch [274/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.01%\n",
      "Epoch [275/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [276/300] - Train Loss: 1.0902 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [277/300] - Train Loss: 1.0896 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.07%\n",
      "Epoch [278/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [279/300] - Train Loss: 1.0894 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [280/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [281/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [282/300] - Train Loss: 1.0901 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.02%\n",
      "Epoch [283/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.03%\n",
      "Epoch [284/300] - Train Loss: 1.0903 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.05%\n",
      "Epoch [285/300] - Train Loss: 1.0903 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [286/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.04%\n",
      "Epoch [287/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.04%\n",
      "Epoch [288/300] - Train Loss: 1.0896 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.07%\n",
      "Epoch [289/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.04%\n",
      "Epoch [290/300] - Train Loss: 1.0896 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.05%\n",
      "Epoch [291/300] - Train Loss: 1.0900 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.01%\n",
      "Epoch [292/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.03%\n",
      "Epoch [293/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.06%\n",
      "Epoch [294/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.05%\n",
      "Epoch [295/300] - Train Loss: 1.0898 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.05%\n",
      "Epoch [296/300] - Train Loss: 1.0899 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.03%\n",
      "Epoch [297/300] - Train Loss: 1.0902 - LR: 0.000000 - Val Loss: 1.0907 - Val Acc: 81.02%\n",
      "Epoch [298/300] - Train Loss: 1.0895 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.08%\n",
      "Epoch [299/300] - Train Loss: 1.0895 - LR: 0.000000 - Val Loss: 1.0906 - Val Acc: 81.06%\n",
      "Epoch [300/300] - Train Loss: 1.0897 - LR: 0.000000 - Val Loss: 1.0905 - Val Acc: 81.03%\n",
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0915, Test Accuracy: 80.94%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0914835617667258, 0.8094084626179903)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
